# src/core/llm_client.py

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from dotenv import load_dotenv
import os
import time
from typing import Optional # Tip hint'leri tutuldu.

# Projenizin konfigÃ¼rasyonunu yÃ¼kle.
from src.core.config import LLM_MODEL_NAME 

load_dotenv()


# ============================================================
# Custom Exception Classes (Graceful Error Handling)
# ============================================================
class QuotaExceededError(Exception):
    """GÃ¼nlÃ¼k API kota limiti dolduÄŸunda fÄ±rlatÄ±lÄ±r."""
    pass


class LLMConnectionError(Exception):
    """Genel LLM baÄŸlantÄ± hatasÄ±."""
    pass


class LLMClient:
    """
    Handles minimal Gemini API calls for Agent operations.
    """

    def __init__(self, model_name: str = LLM_MODEL_NAME):
        self.model_name = model_name

        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("ERROR: GOOGLE_API_KEY is missing in .env file!")

        genai.configure(api_key=api_key)
        self.model: Optional[genai.GenerativeModel] = None
        
        try:
             # Modeli baÅŸlat
             self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:
             error_msg = str(e)
             # Provide helpful error message for model issues
             if "not found" in error_msg.lower() or "invalid" in error_msg.lower():
                 suggestion = (
                     f"\nğŸ’¡ Model '{self.model_name}' bulunamadÄ± veya eriÅŸilemiyor.\n"
                     f"   Ã‡alÄ±ÅŸan alternatifler:\n"
                     f"   - 'gemini-1.5-flash' (Ã¶nerilen, Ã¼cretsiz tier)\n"
                     f"   - 'gemini-pro' (eski ama stabil)\n"
                     f"   - 'gemini-1.5-pro' (faturalandÄ±rma gerekebilir)\n"
                     f"   src/core/config.py dosyasÄ±nda LLM_MODEL_NAME'i deÄŸiÅŸtirin."
                 )
                 raise RuntimeError(f"FATAL: Gemini modeli '{self.model_name}' baÅŸlatÄ±lamadÄ±.\n{error_msg}{suggestion}")
             elif "billing" in error_msg.lower() or "quota" in error_msg.lower():
                 suggestion = (
                     f"\nğŸ’¡ Model '{self.model_name}' faturalandÄ±rma gerektiriyor olabilir.\n"
                     f"   Ãœcretsiz tier iÃ§in 'gemini-1.5-flash' veya 'gemini-pro' kullanÄ±n.\n"
                     f"   src/core/config.py dosyasÄ±nda LLM_MODEL_NAME'i deÄŸiÅŸtirin."
                 )
                 raise RuntimeError(f"FATAL: Gemini modeli '{self.model_name}' baÅŸlatÄ±lamadÄ±.\n{error_msg}{suggestion}")
             else:
                 raise RuntimeError(f"FATAL: Gemini modeli '{self.model_name}' baÅŸlatÄ±lamadÄ±. Hata: {e}")
             
    # ------------------------------------------------------------------
    # ANA METOT: Agent'larÄ±n Ã§aÄŸÄ±rdÄ±ÄŸÄ± sade metot (tek Ã§aÄŸrÄ±)
    # ------------------------------------------------------------------
    def generate_content(self, prompt: str, max_retries: int = 0, stream: bool = False) -> str:
        """
        Verilen prompt ile modelden iÃ§erik Ã¼retir. 
        429 quota hatalarÄ±nda gracefully fail eder (sÃ¼rekli retry yapmaz).
        max_retries=0: Kota dolduÄŸunda hemen durdur (varsayÄ±lan).
        stream=True: Streaming yanÄ±t (progress iÃ§in, ama toplam sÃ¼re aynÄ±)
        """
        
        if self.model is None:
             raise RuntimeError("Gemini modeli kullanÄ±ma hazÄ±r deÄŸil.")
        
        import sys
        
        try:
            if stream:
                # Streaming mode: Progress gÃ¶ster ama toplam sÃ¼re aynÄ±
                print("[LLM] Generating...", end="", flush=True)
                response = self.model.generate_content(prompt, stream=True)
                chunks = []
                for chunk in response:
                    if chunk.text:
                        chunks.append(chunk.text)
                        print(".", end="", flush=True)  # Progress indicator
                print(" âœ“", flush=True)
                return "".join(chunks)
            else:
                # Normal mode: Tek seferde al (daha hÄ±zlÄ±)
                response = self.model.generate_content(prompt)
                return response.text
            
        except google_exceptions.ResourceExhausted as e:
            # 429 quota/rate limit hatasÄ±
            error_msg = str(e)
            
            # Rate limit mi yoksa daily quota mu?
            is_rate_limit = "rate limit" in error_msg.lower() or "quota" not in error_msg.lower()
            
            if is_rate_limit:
                # Rate limit - kÄ±sa sÃ¼re bekle ve retry yap
                import re
                retry_seconds = 5  # Default 5 saniye
                match = re.search(r'retry in (\d+(?:\.\d+)?)s', error_msg, re.IGNORECASE)
                if match:
                    retry_seconds = float(match.group(1))
                    retry_seconds = min(retry_seconds, 60)  # Max 60 saniye
                
                # Rate limit hatasÄ± - LLMConnectionError olarak fÄ±rlat (retry edilebilir)
                raise LLMConnectionError(
                    f"Rate limit reached. Retry after {retry_seconds:.0f} seconds. "
                    f"Error: {error_msg}"
                )
            else:
                # Daily quota doldu - QuotaExceededError fÄ±rlat (dur)
                import re
                retry_delay_str = "bilinmeyen sÃ¼re"
                match = re.search(r'retry in (\d+(?:\.\d+)?)s', error_msg, re.IGNORECASE)
                if match:
                    retry_seconds = float(match.group(1))
                    retry_hours = retry_seconds / 3600
                    retry_delay_str = f"{retry_hours:.1f} saat" if retry_hours >= 1 else f"{retry_seconds:.0f} saniye"
                
                raise QuotaExceededError(
                    f"\nâ›” Gemini API gÃ¼nlÃ¼k kota limiti doldu!\n"
                    f"   YaklaÅŸÄ±k {retry_delay_str} sonra tekrar deneyebilirsiniz.\n"
                    f"   Alternatif: .env dosyanÄ±za yeni bir GOOGLE_API_KEY ekleyin.\n"
                    f"   Detay: {error_msg}"
                )
                
        except Exception as e:
            # DiÄŸer hatalar iÃ§in detaylÄ± mesaj
            raise LLMConnectionError(f"Gemini API Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z oldu: {str(e)}")


# Ã–NEMLÄ° NOT: DiÄŸer JSON veya RAG odaklÄ± metotlarÄ± (llm_json gibi) bu sÄ±nÄ±fa 
# projeniz gerektirdikÃ§e ekleyebilirsiniz.